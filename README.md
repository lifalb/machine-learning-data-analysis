# Machine Learning course 
This is solved homeworks from Machine Learning course that aims to introduce to modern state of Machine Learning.

## Repository structure

* folder `week 1` is about linear regression and gradient descent 
* folder `week 2` is about metrics in classification tasks and how to overfitting models
* folder `week 3` is about basics of data preprocessing and logistic regressio
* folder `week 4` is about how to build rf and gbt
* folder `week 5` is about naive bayes classifier

## Materials for self-study

### Prerequisites

1. [en] Stanford lectures on Probability Theory:
   [link](https://web.stanford.edu/~montanar/TEACHING/Stat310A/lnotes.pdf)
1. [en] Matrix calculus notes from Stanford:
   [link](http://cs231n.stanford.edu/vecDerivs.pdf)
1. [en] Derivatives notes from Stanford:
   [link](http://cs231n.stanford.edu/handouts/derivatives.pdf)

### Basic Machine Learning

1. [en] The Hundred-page Machine Learning book: [link](http://themlbook.com)
   (available online, e.g. on the
   [github](https://github.com/ZakiaSalod/The-Hundred-Page-Machine-Learning-Book))
1. [en] Naive Bayesian classifier explained:
   [link](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)
1. [en] Stanford notes on linear models:
   [link](http://cs229.stanford.edu/notes/cs229-notes1.pdf)

### Gradient Boosting and Feature importances

1. [en] Great interactive blogpost by Alex Rogozhnikov on Gradient Boosting:
   http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html
1. [en] And great gradient boosted trees playground by Alex Rogozhnikov:
   http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html